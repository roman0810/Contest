{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d02cb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import itertools\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4105a7ed-125e-440f-ac0d-eb08025cdd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 2.15 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# Типа пайплайн первичных данных\n",
    "path1 = \"/Users/romanvisotsky/Downloads/data/main1.csv\"\n",
    "path2 = \"/Users/romanvisotsky/Downloads/data/main2.csv\"\n",
    "path3 = \"/Users/romanvisotsky/Downloads/data/main3.csv\"\n",
    "\n",
    "df = pd.read_csv(path1)\n",
    "df_2 = pd.read_csv(path2)\n",
    "df_3 = pd.read_csv(path3)\n",
    "\n",
    "def clean_fio(fio):\n",
    " \n",
    "    # Убираем цифры между буквами (без удаления пробелов)\n",
    "    fio = re.sub(r'(\\D)\\d+(\\D)', r'\\1\\2', fio)\n",
    "    # Удаляем оставшиеся цифры (в начале и в конце)\n",
    "    fio = re.sub(r'\\d+', '', fio)\n",
    "    fio = re.sub(r'[^\\w\\s]', '', fio) #Удаление всех символов, кроме букв (и цифр, но их мы уже удалили)\n",
    " \n",
    "    fio = re.sub(r'\\b[А-ЯA-Z]{2}[а-яa-z]+\\b', '', fio) #Удаление всех слов где начало с 2х заглавных, а потом обычные\n",
    "    fio = re.sub(r'\\b[А-ЯA-Z][а-яa-z]+\\b', '', fio) #Тоже самое с 1 заглавной\n",
    "    fio = re.sub(r'\\b[а-яa-z]+\\b', '', fio) # Только строчные\n",
    " \n",
    "    fio = re.sub(r'\\b[оглиыуОГЛИЫУoO]+\\b', '', fio, flags=re.IGNORECASE) #Удаляем угли, углы, огли, оглы (точнее любые слова состоящие только из сочетаний из этих букв)\n",
    " \n",
    "    words = fio.split()\n",
    "    unique_words = list(dict.fromkeys(words))  # Сохраняем порядок и удаляем дубликаты\n",
    "    fio = ' '.join(unique_words)  # Обратно соединяем слова в строку\n",
    " \n",
    "    # Убираем лишние пробелы\n",
    "    fio = fio.strip()\n",
    "    fio = re.sub(r'\\s+', ' ', fio)\n",
    " \n",
    "    fio = fio.upper()\n",
    "    \n",
    "    return fio\n",
    " \n",
    "def clean_and_merge_fio(df):\n",
    "    # Объединяем части ФИО и заполняем NaN пустыми строками\n",
    "    df['full_name'] = df['last_name'].fillna('') + ' ' + df['first_name'].fillna('') + ' ' + df['middle_name'].fillna('')\n",
    " \n",
    "    # Удаляем дублирующиеся буквы\n",
    "    df['full_name'] = df['full_name'].str.replace(r'(.)\\1+', r'\\1', regex=True)\n",
    " \n",
    "    # Удаление цифр и специальных символов\n",
    "    df['full_name'] = df['full_name'].str.replace(r'(\\D)\\d+(\\D)', r'\\1\\2', regex=True)\n",
    "    df['full_name'] = df['full_name'].str.replace(r'\\d+', '', regex=True)\n",
    "    df['full_name'] = df['full_name'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
    " \n",
    "    # Удаляем слова, содержащие \"нет\" или \"отсутствует\"\n",
    "    df['full_name'] = df['full_name'].apply(lambda x: ' '.join(word for word in x.split() if not re.search(r'\\b(нет|отсутствует)\\b', word, flags=re.IGNORECASE)))\n",
    " \n",
    "    # Удаляем нежелательные слова\n",
    "    df['full_name'] = df['full_name'].str.replace(r'\\b[А-ЯA-Z]{2}[а-яa-z]+\\b', '', regex=True)\n",
    "    df['full_name'] = df['full_name'].str.replace(r'\\b[А-ЯA-Z][а-яa-z]+\\b', '', regex=True)\n",
    "    df['full_name'] = df['full_name'].str.replace(r'\\b[а-яa-z]+\\b', '', regex=True)\n",
    "    df['full_name'] = df['full_name'].str.replace(r'\\b[оглиыуОГЛИЫУoO]+\\b', '', regex=True, flags=re.IGNORECASE)\n",
    " \n",
    "    # Удаление дубликатов и сохранение порядка\n",
    "    df['full_name'] = df['full_name'].apply(lambda x: ' '.join(dict.fromkeys(x.split())))\n",
    " \n",
    "    # Убираем лишние пробелы и приводим к верхнему регистру\n",
    "    df['full_name'] = df['full_name'].str.replace(r'\\s+', ' ', regex=True).str.strip().str.upper()\n",
    " \n",
    "    # Заменяем пустые строки на NaN\n",
    "    df['full_name'] = df['full_name'].replace('', np.nan)\n",
    " \n",
    "    return df\n",
    " \n",
    " \n",
    "def process_birthdates(birthdate_series):\n",
    "    def correct_date(date_str):\n",
    "        # Удаляем символы до первой цифры\n",
    "        date_str = re.sub(r'^[^\\d]*', '', date_str)\n",
    " \n",
    "        # Проверяем год\n",
    "        year_match = re.match(r'^(\\d+)', date_str)\n",
    "        if year_match:\n",
    "            year_str = year_match.group(0)\n",
    "            year = int(year_str)\n",
    "            year_length = len(year_str)\n",
    " \n",
    "            # Проверка на валидность года перед изменениями\n",
    "            if 1900 <= year <= 2024:\n",
    "                return date_str  # Год валиден, возвращаем строку\n",
    " \n",
    "            if year_length > 4:\n",
    "                if year_str.startswith('0'):\n",
    "                    year_str = year_str[1:]  # Удаляем первую цифру, если 0\n",
    "                elif year_str.startswith('10'):\n",
    "                    year_str = year_str[0] + year_str[2:]  # Удаляем 0 после 1\n",
    " \n",
    "                year = int(year_str)\n",
    " \n",
    "            # Проверка на валидность года перед изменениями\n",
    "                if 1900 <= year <= 2024:\n",
    "                    date_str = date_str.replace(year_match.group(0), str(year), 1)\n",
    "                    return date_str  # Год валиден, возвращаем строку\n",
    " \n",
    "            if year_length == 3:\n",
    "                year_str = '1' + year_str  # Добавляем 1 в начало\n",
    "                year = int(year_str)\n",
    " \n",
    "                # Проверка на валидность года\n",
    "                if 1900 <= year <= 2024:\n",
    "                    date_str = date_str.replace(year_match.group(0), str(year), 1)\n",
    "                    return date_str  # Год валиден, возвращаем строку\n",
    " \n",
    "            # Обработка 4-значного года\n",
    "            if len(year_str) == 4:\n",
    "                # Если год меньше 1900 и 2, 3 цифра равны 0, меняем первую цифру на 2\n",
    "                if year < 1900 and len(str(year)) >= 3 and str(year)[1] == '0' and (str(year)[2] == '0' or str(year)[2] == '1'):\n",
    "                    year_str = '2' + str(year)[1:]  # Изменение первой цифры\n",
    "                    year = int(year_str)\n",
    " \n",
    "                if year < 1900:\n",
    "                    year_str = year_str[0] + '9' + year_str[2:]# Изменение первой цифры\n",
    "                    year = int(year_str)\n",
    " \n",
    "                # Если год больше 2024 и вторая цифра 9, изменяем первую цифру\n",
    "                if year > 2024 and len(str(year)) >= 2 and str(year)[1] == '9':\n",
    "                    year_str = '1' + str(year)[1:]  # Изменение первой цифры\n",
    "                    year = int(year_str)\n",
    " \n",
    " \n",
    "                # Проверяем валидность года после изменения\n",
    "                if 1900 <= year <= 2024:\n",
    "                    date_str = date_str.replace(year_match.group(0), str(year), 1)\n",
    "                    return date_str\n",
    " \n",
    "                # Если год не валиден, пробуем перестановки\n",
    "                permutations = set(itertools.permutations(year_str))\n",
    "                valid_years = [int(''.join(p)) for p in permutations if 1900 <= int(''.join(p)) <= 2024]\n",
    "                if valid_years:\n",
    "                    year = max(valid_years)\n",
    "                    date_str = date_str.replace(year_match.group(0), str(year), 1)\n",
    "                    return date_str\n",
    " \n",
    "            elif year_length == 2:\n",
    "                if year <= 24:  # Предполагаем, что 00-24 это 2000-2024\n",
    "                    year_str = '20' + year_str\n",
    "                else:  # Предполагаем, что 25-99 это 1925-1999\n",
    "                    year_str = '19' + year_str\n",
    "                year = int(year_str)\n",
    "                date_str = date_str.replace(year_match.group(0), str(year), 1)\n",
    "                return date_str\n",
    " \n",
    "        # Если год не валиден, возвращаем измененную строку\n",
    "        return date_str\n",
    " \n",
    "    # Применяем обработку ко всей колонке\n",
    "    return birthdate_series.apply(correct_date)\n",
    " \n",
    " \n",
    "df['full_name'] = df['full_name'].apply(clean_fio)\n",
    "df_3['name'] = df_3['name'].apply(clean_fio)\n",
    "clean_and_merge_fio(df_2)\n",
    " \n",
    "df['birthdate'] = process_birthdates(df['birthdate'])\n",
    "df_2['birthdate'] = process_birthdates(df_2['birthdate'])\n",
    "df_3['birthdate'] = process_birthdates(df_3['birthdate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d71089ab-1370-4c28-abdb-a875bddb29ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# типа пайплайн маркировки\n",
    "\n",
    "def clean_phone_number(phone):\n",
    "    phone = re.sub(r'[^\\d+]', '', phone)\n",
    "    if phone.startswith('+'):\n",
    "        phone = '+' + re.sub(r'[^\\d]', '', phone[1:])\n",
    "    else:\n",
    "        phone = re.sub(r'[^\\d]', '', phone)\n",
    "    return phone\n",
    " \n",
    "def random_noise_for_fio(full_name, tr = 0):\n",
    "    def random_letter_modification(word):\n",
    "        if random.random() < 0.5:\n",
    "            if len(word) > 1:\n",
    "                idx = random.randint(0, len(word) - 1)\n",
    "                word = word[:idx] + word[idx+1:]\n",
    "        else:\n",
    "            idx = random.randint(0, len(word))\n",
    "            word = word[:idx] + random.choice(\"АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ\") + word[idx:]\n",
    "        return word\n",
    " \n",
    "    def random_word_swap(words):\n",
    "        if len(words) > 1:\n",
    "            idx1, idx2 = random.sample(range(len(words)), 2)\n",
    "            words[idx1], words[idx2] = words[idx2], words[idx1]\n",
    "        return words\n",
    " \n",
    "    name_parts = full_name.split()\n",
    " \n",
    "    if len(name_parts) > 2 and (random.random() < 0.1 or tr == 1):\n",
    "        name_parts = name_parts[:2]\n",
    " \n",
    "    name_parts = [random_letter_modification(part) for part in name_parts]\n",
    "    \n",
    "    if random.random() < 0.3:\n",
    "        name_parts = random_word_swap(name_parts)\n",
    " \n",
    "    noisy_name = \" \".join(name_parts)\n",
    "    return noisy_name\n",
    " \n",
    " \n",
    "def random_noise_for_email(email):\n",
    "    \n",
    "    similar_letters = {'o': '0', 'l': '1', 'i': '1', 'e': '3', 'a': '4'}\n",
    "    index = random.randint(0, len(email) - 2)\n",
    "    if email[index] in similar_letters:\n",
    "        email = email[:index] + similar_letters[email[index]] +  email[index + 1:]\n",
    " \n",
    "    if random.random() < 1/3:\n",
    "        if '@' in email:\n",
    "            parts = email.split('@')\n",
    "            email = parts[0]\n",
    "            if random.choice([True, False]):\n",
    "                email += '@' \n",
    "    \n",
    "    return email\n",
    " \n",
    "def random_noise_for_birthdate(birthdate):\n",
    "    if random.choice([True, False]):\n",
    "        year = int(birthdate[:4])\n",
    "        new_year = year + random.randint(-1000, 1000)\n",
    "        birthdate = str(new_year) + birthdate[4:]\n",
    "    else:\n",
    "        index = random.randint(0, len(birthdate) - 1)\n",
    "        random_digit = random.choice(string.digits)\n",
    "        birthdate =  birthdate[:index] + random_digit + birthdate[index + 1:]\n",
    "    \n",
    "    return birthdate\n",
    " \n",
    "def random_noise_for_phone(phone):\n",
    "    def random_digit_modification(number):\n",
    "        if random.random() < 0.5:\n",
    "            if len(number) > 1:\n",
    "                idx = random.randint(0, len(number) - 1)\n",
    "                number = number[:idx] + number[idx+1:]\n",
    "        else:\n",
    "            idx = random.randint(0, len(number))\n",
    "            number = number[:idx] + str(random.randint(0, 9)) + number[idx:]\n",
    "        return number\n",
    "    phone = random_digit_modification(clean_phone_number(phone))\n",
    "    \n",
    "    return phone\n",
    " \n",
    "def apply_noise(data, columns_to_noise, tr = 0):\n",
    "    for column in columns_to_noise:\n",
    "        try:\n",
    "            if column in ['full_name', 'name']:\n",
    "                data[column] = random_noise_for_fio(data[column], tr)\n",
    "            elif column == 'email':\n",
    "                data[column] = random_noise_for_email(data[column])\n",
    "    \n",
    "            elif column == 'birthdate':\n",
    "                data[column] = random_noise_for_birthdate(data[column])\n",
    "            elif column == 'phone':\n",
    "                data[column] = random_noise_for_phone(data[column])\n",
    "                \n",
    "        except: pass\n",
    "            \n",
    "    return data\n",
    " \n",
    "def apply_noise_to_date(dates):\n",
    "    base_date = []\n",
    "    new_date = []\n",
    "    false_date = []\n",
    "    \n",
    "    for i in range(len(dates)):  \n",
    "        row = dates.iloc[i] \n",
    "        row2 = dates.iloc[random.randint(1,len(dates)-1)] \n",
    "        r = random.random()\n",
    "        r2 = random.random()\n",
    "        r3 = random.random()\n",
    "        \n",
    "        if r < 1/3: \n",
    "            columns_to_noise = random.sample([col for col in data.columns if col not in ['uid', 'address', 'phone']], 1)\n",
    "            apply_noise(row, columns_to_noise, tr=1)  \n",
    "            new_date.append(f\"{row['full_name']}, {row['email']}, {row['sex']}, {row['birthdate']}, 00000000, \")\n",
    "        elif r < 2/3:\n",
    "            columns_to_noise = random.sample([col for col in data.columns if col not in ['uid', 'address', 'email', 'sex']], 1)\n",
    "            apply_noise(row, columns_to_noise)  \n",
    "            new_date.append(f\"{row['full_name']}, 00000000, 00000000, {row['birthdate']}, {row['phone']}\")\n",
    "        else:\n",
    "            columns_to_noise = random.sample([col for col in data.columns if col not in ['uid', 'address']], 1)\n",
    "            apply_noise(row, columns_to_noise)  \n",
    "            new_date.append(f\"{row['full_name']}, {row['email']}, {row['sex']}, {row['birthdate']}, {row['phone']}\")\n",
    "            \n",
    "        if r2 < 1/3: \n",
    "            columns_to_noise = random.sample([col for col in data.columns if col not in ['uid', 'address', 'phone']], 1)\n",
    "            apply_noise(row2, columns_to_noise, tr=1)  \n",
    "            false_date.append(f\"{row2['full_name']}, {row2['email']}, {row2['sex']}, {row2['birthdate']}, 00000000, \")\n",
    "        elif r2 < 2/3:\n",
    "            columns_to_noise = random.sample([col for col in data.columns if col not in ['uid', 'address', 'email', 'sex']], 1)\n",
    "            apply_noise(row2, columns_to_noise)  \n",
    "            false_date.append(f\"{row2['full_name']}, 00000000, 00000000, {row2['birthdate']}, {row2['phone']}\")\n",
    "        else:\n",
    "            columns_to_noise = random.sample([col for col in data.columns if col not in ['uid', 'address']], 1)\n",
    "            apply_noise(row2, columns_to_noise)  \n",
    "            false_date.append(f\"{row2['full_name']}, {row2['email']}, {row2['sex']}, {row2['birthdate']}, {row2['phone']}\")\n",
    " \n",
    "        if r3 < 1/3: \n",
    "            columns_to_noise = random.sample([col for col in data.columns if col not in ['uid', 'address', 'phone']], 1)\n",
    "            apply_noise(row, columns_to_noise, tr=1)  \n",
    "            base_date.append(f\"{row['full_name']}, {row['email']}, {row['sex']}, {row['birthdate']}, 00000000, \")\n",
    "        elif r3 < 2/3:\n",
    "            columns_to_noise = random.sample([col for col in data.columns if col not in ['uid', 'address', 'email', 'sex']], 1)\n",
    "            apply_noise(row, columns_to_noise)  \n",
    "            base_date.append(f\"{row['full_name']}, 00000000, 00000000, {row['birthdate']}, {row['phone']}\")\n",
    "        else:\n",
    "            columns_to_noise = random.sample([col for col in data.columns if col not in ['uid', 'address']], 1)\n",
    "            apply_noise(row, columns_to_noise)  \n",
    "            base_date.append(f\"{row['full_name']}, {row['email']}, {row['sex']}, {row['birthdate']}, {row['phone']}\")\n",
    "    \n",
    "    return base_date, new_date, false_date\n",
    "        \n",
    " \n",
    "data = df.head(100000).copy()\n",
    " \n",
    "# base_date, new_data, false_date = apply_noise_to_date(data)\n",
    "dataset = apply_noise_to_date(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d53ba08e-45de-4da3-ae21-c756255b4f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict = {\n",
    "    '$':  0.0,\n",
    "    '/':  40.0,\n",
    "    '+':  39.0,\n",
    "    '(':  38.0,\n",
    "    ')':  37.0,\n",
    "    ',':  36.0,\n",
    "    ' ':  35.0,\n",
    "    '_':  34.0,\n",
    "    'о':  33.0,\n",
    "    'е':  32.0,\n",
    "    'ё':  31.0,\n",
    "    'а':  30.0,\n",
    "    'и':  29.0,\n",
    "    'н':  28.0,\n",
    "    'т':  27.0,\n",
    "    'с':  26.0,\n",
    "    'р':  25.0,\n",
    "    'в':  24.0,\n",
    "    'л':  23.0,\n",
    "    'к':  22.0,\n",
    "    'м':  21.0,\n",
    "    'д':  20.0,\n",
    "    'п':  19.0,\n",
    "    'у':  18.0,\n",
    "    'я':  17.0,\n",
    "    'з':  16.0,\n",
    "    'ы':  15.0,\n",
    "    'б':  14.0,\n",
    "    'ь':  13.0,\n",
    "    'ъ':  12.0,\n",
    "    'г':  11.0,\n",
    "    'ч':  10.0,\n",
    "    'й':  9.0,\n",
    "    'х':  8.0,\n",
    "    'ж':  7.0,\n",
    "    'ш':  6.0,\n",
    "    'ю':  5.0,\n",
    "    'ц':  4.0,\n",
    "    'щ':  3.0,\n",
    "    'э':  2.0,\n",
    "    'ф':  1.0,\n",
    "    '.':  -41.0,\n",
    "    '0':  -40.0,\n",
    "    '1':  -39.0,\n",
    "    '2':  -38.0,\n",
    "    '3':  -37.0,\n",
    "    '4':  -36.0,\n",
    "    '5':  -35.0,\n",
    "    '6':  -34.0,\n",
    "    '7':  -33.0,\n",
    "    '8':  -32.0,\n",
    "    '9':  -31.0,\n",
    "    '\"':  -30.0,\n",
    "    '-':  -29.0,\n",
    "    ';':  -28.0,\n",
    "    '@':  -27.0,\n",
    "    'e':  -26.0,\n",
    "    't':  -25.0,\n",
    "    'a':  -24.0,\n",
    "    'o':  -23.0,\n",
    "    'n':  -22.0,\n",
    "    'i':  -21.0,\n",
    "    's':  -20.0,\n",
    "    'r':  -19.0,\n",
    "    'h':  -18.0,\n",
    "    'l':  -17.0,\n",
    "    'd':  -16.0,\n",
    "    'c':  -15.0,\n",
    "    'u':  -14.0,\n",
    "    'p':  -13.0,\n",
    "    'f':  -12.0,\n",
    "    'm':  -11.0,\n",
    "    'w':  -10.0,\n",
    "    'y':  -9.0,\n",
    "    'b':  -8.0,\n",
    "    'g':  -7.0,\n",
    "    'v':  -6.0,\n",
    "    'k':  -5.0,\n",
    "    'q':  -4.0,\n",
    "    'x':  -3.0,\n",
    "    'j':  -2.0,\n",
    "    'z':  -1.0\n",
    "}\n",
    "\n",
    "def tokenizer(S):\n",
    "    if len(S)<64:\n",
    "        S+=\"$\"*(64-len(S))\n",
    "    return torch.tensor([token_dict[i] for i in S.lower()]).unsqueeze(0).view((-1,1))/41.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19f9198c-3c88-416d-81f9-1cd81e5562b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# эталонный размер тензоров похожести\n",
    "# АХТУНГ: исходные тензоры ОБЯЗАНЫ иметь размер НЕ МЕНЬШИЙ эталонного\n",
    "Tensor_Standart = (64,64)\n",
    "\n",
    "class SimilarityTensor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimilarityTensor, self).__init__()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Linear(in_features = 1, out_features = 16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features = 16, out_features = 32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features = 32, out_features = 32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features = 32, out_features = 16)\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Linear(in_features = 16, out_features = 32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features = 32, out_features = 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features = 64, out_features = 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features = 64, out_features = 16)\n",
    "        )\n",
    "\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Linear(in_features = 16, out_features = 32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features = 32, out_features = 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features = 64, out_features = 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features = 64, out_features = 16) \n",
    "        )\n",
    "\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Linear(in_features = 16, out_features = 32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features = 32, out_features = 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features = 64, out_features = 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features = 64, out_features = 16) \n",
    "        )\n",
    "\n",
    "        self.FC1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 4, out_channels = 32, kernel_size = 4, stride = 2, padding=0, bias = True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 4, stride = 2, padding=0, bias = True),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 4, stride = 2, padding=0, bias = True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(in_channels = 64, out_channels = 48, kernel_size = 4, stride = 2, padding=0, bias = True),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.FC2 = nn.Sequential(\n",
    "            nn.Linear(in_features = 192, out_features = 400),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features = 400, out_features = 200),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features = 200, out_features = 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    # получение пуллинга приводящего тезор к эталонному формату по осям -1 и -2\n",
    "    def get_pool(self, cur_shape:tuple, target_shape: tuple):\n",
    "        if cur_shape[-1] < target_shape[-1] or cur_shape[-2] < target_shape[-2]:\n",
    "            raise ValueError('target shape mast be smaller then current')\n",
    "    \n",
    "        kernel_size = (cur_shape[-2]%(target_shape[-2]-1),cur_shape[-1]%(target_shape[-1]-1))\n",
    "        stride = (cur_shape[-2]//(target_shape[-2]-1),cur_shape[-1]//(target_shape[-1]-1))\n",
    "        return nn.AvgPool2d(kernel_size, stride)\n",
    "\n",
    "    def forward(self, S_b, S_t, S_f):\n",
    "        # Создаем тензоры похожести\n",
    "        #--- 1 уровнь эмбедингов\n",
    "        S_b = self.conv1(S_b)\n",
    "        S_t = self.conv1(S_t)\n",
    "        S_f = self.conv1(S_f)\n",
    "\n",
    "        # создаем матрицы со скалярными произведениями каждой пары эмбедингов\n",
    "        BT_embeding = torch.matmul(S_t.view((-1,16)), torch.transpose(S_b.view((-1,16)), -2,-1))\n",
    "        BF_embeding = torch.matmul(S_f.view((-1,16)), torch.transpose(S_b.view((-1,16)), -2,-1))\n",
    "\n",
    "        # АХТУНГ: инплейс операция\n",
    "        # Добавляем матрицам похожести новое измерение для сложения единого тензора из них\n",
    "        BT_embeding.unsqueeze_(0)\n",
    "        BF_embeding.unsqueeze_(0)\n",
    "\n",
    "        #--- 2 уровнь эмбедингов\n",
    "        S_b = self.conv2(S_b)\n",
    "        S_t = self.conv2(S_t)\n",
    "        S_f = self.conv2(S_f)\n",
    "\n",
    "        # матрицы похожести 2 уровня эмбедингов\n",
    "        BT_embeding_onstep = torch.matmul(S_t.view((-1,16)), torch.transpose(S_b.view((-1,16)), -2,-1))\n",
    "        BF_embeding_onstep = torch.matmul(S_f.view((-1,16)), torch.transpose(S_b.view((-1,16)), -2,-1))\n",
    "\n",
    "        BT_embeding_onstep.unsqueeze_(0)\n",
    "        BF_embeding_onstep.unsqueeze_(0)\n",
    "\n",
    "        # соединяем матрицы похожести\n",
    "        BT_embeding = torch.cat((BT_embeding,BT_embeding_onstep),0)\n",
    "        BF_embeding = torch.cat((BF_embeding,BF_embeding_onstep),0)\n",
    "        \n",
    "        #--- 3 уровнь эмбедингов\n",
    "        S_b = self.conv3(S_b)\n",
    "        S_t = self.conv3(S_t)\n",
    "        S_f = self.conv3(S_f)\n",
    "        \n",
    "        # матрицы похожести 3 уровня эмбедингов\n",
    "        BT_embeding_onstep = torch.matmul(S_t.view((-1,16)), torch.transpose(S_b.view((-1,16)), -2,-1))\n",
    "        BF_embeding_onstep = torch.matmul(S_f.view((-1,16)), torch.transpose(S_b.view((-1,16)), -2,-1))\n",
    "\n",
    "        BT_embeding_onstep.unsqueeze_(0)\n",
    "        BF_embeding_onstep.unsqueeze_(0)\n",
    "\n",
    "        BT_embeding = torch.cat((BT_embeding,BT_embeding_onstep),0)\n",
    "        BF_embeding = torch.cat((BF_embeding,BF_embeding_onstep),0)\n",
    "\n",
    "        #--- 4 уровнь эмбедингов\n",
    "        S_b = self.conv4(S_b)\n",
    "        S_t = self.conv4(S_t)\n",
    "        S_f = self.conv4(S_f)\n",
    "\n",
    "        BT_embeding_onstep = torch.matmul(S_t.view((-1,16)), torch.transpose(S_b.view((-1,16)), -2,-1))\n",
    "        BF_embeding_onstep = torch.matmul(S_f.view((-1,16)), torch.transpose(S_b.view((-1,16)), -2,-1))\n",
    "\n",
    "        BT_embeding_onstep.unsqueeze_(0)\n",
    "        BF_embeding_onstep.unsqueeze_(0)\n",
    "\n",
    "        BT_embeding = torch.cat((BT_embeding,BT_embeding_onstep),0)\n",
    "        BF_embeding = torch.cat((BF_embeding,BF_embeding_onstep),0)\n",
    "        \n",
    "        # На основании тензоров похожести ищем ими описанное расстояние\n",
    "        # Приводим тензоры к эталонному размеру\n",
    "        BT_pool = self.get_pool(BT_embeding.shape, Tensor_Standart)\n",
    "        BF_pool = self.get_pool(BF_embeding.shape, Tensor_Standart)\n",
    "\n",
    "        BT_embeding = BT_pool(BT_embeding)\n",
    "        BF_embeding = BF_pool(BF_embeding)\n",
    "\n",
    "        # пропускаем тензоры 4х64х64 через свертки\n",
    "        # для сверточного слоя тензор должен иметь 4D shape, приводим к (1,4,64,64)\n",
    "        BT_embeding.unsqueeze_(0)\n",
    "        BF_embeding.unsqueeze_(0)\n",
    "        \n",
    "        BT_embeding = self.FC1(BT_embeding)\n",
    "        BF_embeding = self.FC1(BF_embeding)\n",
    "\n",
    "        # линеаризуем результат\n",
    "        BT_embeding = self.flatten(BT_embeding)\n",
    "        BF_embeding = self.flatten(BF_embeding)\n",
    "\n",
    "        # извлекаем меру похожести между BT и BF\n",
    "        BT_embeding = self.FC2(BT_embeding)\n",
    "        BF_embeding = self.FC2(BF_embeding)\n",
    "\n",
    "        return BT_embeding, BF_embeding\n",
    "\n",
    "    def get_dis(self, S_b, S_t):\n",
    "        #--- 1 уровнь эмбедингов\n",
    "        S_b = self.conv1(S_b)\n",
    "        S_t = self.conv1(S_t)\n",
    "\n",
    "        # создаем матрицу со скалярными произведениями каждой пары эмбедингов\n",
    "        BT_embeding = torch.matmul(S_t.view((-1,16)), torch.transpose(S_b.view((-1,16)), -2,-1))\n",
    "\n",
    "        # Добавляем матрице похожести новое измерение для сложения единого тензора\n",
    "        BT_embeding.unsqueeze_(0)\n",
    "\n",
    "        #--- 2 уровнь эмбедингов\n",
    "        S_b = self.conv2(S_b)\n",
    "        S_t = self.conv2(S_t)\n",
    "\n",
    "        # матрицы похожести 2 уровня эмбедингов\n",
    "        BT_embeding_onstep = torch.matmul(S_t.view((-1,16)), torch.transpose(S_b.view((-1,16)), -2,-1))\n",
    "\n",
    "        BT_embeding_onstep.unsqueeze_(0)\n",
    "\n",
    "        # соединяем матрицы похожести\n",
    "        BT_embeding = torch.cat((BT_embeding,BT_embeding_onstep),0)\n",
    "\n",
    "        #--- 3 уровнь эмбедингов\n",
    "        S_b = self.conv3(S_b)\n",
    "        S_t = self.conv3(S_t)\n",
    "\n",
    "        # матрицы похожести 3 уровня эмбедингов\n",
    "        BT_embeding_onstep = torch.matmul(S_t.view((-1,16)), torch.transpose(S_b.view((-1,16)), -2,-1))\n",
    "        \n",
    "        BT_embeding_onstep.unsqueeze_(0)\n",
    "\n",
    "        BT_embeding = torch.cat((BT_embeding,BT_embeding_onstep),0)\n",
    "\n",
    "        #--- 4 уровнь эмбедингов\n",
    "        S_b = self.conv4(S_b)\n",
    "        S_t = self.conv4(S_t)\n",
    "\n",
    "        BT_embeding_onstep = torch.matmul(S_t.view((-1,16)), torch.transpose(S_b.view((-1,16)), -2,-1))\n",
    "        \n",
    "        BT_embeding_onstep.unsqueeze_(0)\n",
    "\n",
    "        BT_embeding = torch.cat((BT_embeding,BT_embeding_onstep),0)\n",
    "\n",
    "        # На основании тензоров похожести ищем ими описанное расстояние\n",
    "        # Приводим тензоры к эталонному размеру\n",
    "        BT_pool = self.get_pool(BT_embeding.shape, Tensor_Standart)\n",
    "\n",
    "        BT_embeding = BT_pool(BT_embeding)\n",
    "\n",
    "        # пропускаем тензоры 4х64х64 через свертки\n",
    "        # для сверточного слоя тензор должен иметь 4D shape, приводим к (1,4,64,64)\n",
    "        BT_embeding.unsqueeze_(0)\n",
    "\n",
    "        BT_embeding = self.FC1(BT_embeding)\n",
    "\n",
    "        # линеаризуем результат\n",
    "        BT_embeding = self.flatten(BT_embeding)\n",
    "\n",
    "        # извлекаем меру похожести между BT и BF\n",
    "        BT_embeding = self.FC2(BT_embeding)\n",
    "\n",
    "        return BT_embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dacfeaa-1b56-42ff-b903-04b5d1fc9c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_margin = torch.tensor([0.1])\n",
    "def criterion(BT_embeding, BF_embeding):\n",
    "    return torch.max(BT_embeding - BF_embeding + alpha_margin, torch.zeros((1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a73eab9-02f5-479e-b888-fce8697b755e",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 'БАЛКИБАЕВ МАМАСАИД КАРЛИТО,mamasaid_balkibaev0@example.ru,m,1954-06-15,0206450526'\n",
    "T = 'АЛКИБАЕВ МАМАСАИД КАРЛИТО,mamasaid_balkibaev0@example.ru,m,2954-06-15,0206450526'\n",
    "F = 'ИГИНОВА ЮТТА КОНДРАТЬЕВНП,jutta_iginova1@yandex.ru,f,1981-11-24,1971286327'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b0c55f66-f48d-413d-9165-79648d666cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1           [-1, 100, 1, 16]              32\n",
      "         LeakyReLU-2           [-1, 100, 1, 16]               0\n",
      "            Linear-3           [-1, 100, 1, 32]             544\n",
      "         LeakyReLU-4           [-1, 100, 1, 32]               0\n",
      "            Linear-5           [-1, 100, 1, 32]           1,056\n",
      "         LeakyReLU-6           [-1, 100, 1, 32]               0\n",
      "            Linear-7           [-1, 100, 1, 16]             528\n",
      "            Linear-8           [-1, 100, 1, 16]              32\n",
      "         LeakyReLU-9           [-1, 100, 1, 16]               0\n",
      "           Linear-10           [-1, 100, 1, 32]             544\n",
      "        LeakyReLU-11           [-1, 100, 1, 32]               0\n",
      "           Linear-12           [-1, 100, 1, 32]           1,056\n",
      "        LeakyReLU-13           [-1, 100, 1, 32]               0\n",
      "           Linear-14           [-1, 100, 1, 16]             528\n",
      "           Linear-15           [-1, 100, 1, 16]              32\n",
      "        LeakyReLU-16           [-1, 100, 1, 16]               0\n",
      "           Linear-17           [-1, 100, 1, 32]             544\n",
      "        LeakyReLU-18           [-1, 100, 1, 32]               0\n",
      "           Linear-19           [-1, 100, 1, 32]           1,056\n",
      "        LeakyReLU-20           [-1, 100, 1, 32]               0\n",
      "           Linear-21           [-1, 100, 1, 16]             528\n",
      "           Linear-22           [-1, 100, 1, 32]             544\n",
      "        LeakyReLU-23           [-1, 100, 1, 32]               0\n",
      "           Linear-24           [-1, 100, 1, 64]           2,112\n",
      "        LeakyReLU-25           [-1, 100, 1, 64]               0\n",
      "           Linear-26           [-1, 100, 1, 64]           4,160\n",
      "        LeakyReLU-27           [-1, 100, 1, 64]               0\n",
      "           Linear-28           [-1, 100, 1, 16]           1,040\n",
      "           Linear-29           [-1, 100, 1, 32]             544\n",
      "        LeakyReLU-30           [-1, 100, 1, 32]               0\n",
      "           Linear-31           [-1, 100, 1, 64]           2,112\n",
      "        LeakyReLU-32           [-1, 100, 1, 64]               0\n",
      "           Linear-33           [-1, 100, 1, 64]           4,160\n",
      "        LeakyReLU-34           [-1, 100, 1, 64]               0\n",
      "           Linear-35           [-1, 100, 1, 16]           1,040\n",
      "           Linear-36           [-1, 100, 1, 32]             544\n",
      "        LeakyReLU-37           [-1, 100, 1, 32]               0\n",
      "           Linear-38           [-1, 100, 1, 64]           2,112\n",
      "        LeakyReLU-39           [-1, 100, 1, 64]               0\n",
      "           Linear-40           [-1, 100, 1, 64]           4,160\n",
      "        LeakyReLU-41           [-1, 100, 1, 64]               0\n",
      "           Linear-42           [-1, 100, 1, 16]           1,040\n",
      "           Linear-43           [-1, 100, 1, 32]             544\n",
      "        LeakyReLU-44           [-1, 100, 1, 32]               0\n",
      "           Linear-45           [-1, 100, 1, 64]           2,112\n",
      "        LeakyReLU-46           [-1, 100, 1, 64]               0\n",
      "           Linear-47           [-1, 100, 1, 64]           4,160\n",
      "        LeakyReLU-48           [-1, 100, 1, 64]               0\n",
      "           Linear-49           [-1, 100, 1, 16]           1,040\n",
      "           Linear-50           [-1, 100, 1, 32]             544\n",
      "        LeakyReLU-51           [-1, 100, 1, 32]               0\n",
      "           Linear-52           [-1, 100, 1, 64]           2,112\n",
      "        LeakyReLU-53           [-1, 100, 1, 64]               0\n",
      "           Linear-54           [-1, 100, 1, 64]           4,160\n",
      "        LeakyReLU-55           [-1, 100, 1, 64]               0\n",
      "           Linear-56           [-1, 100, 1, 16]           1,040\n",
      "           Linear-57           [-1, 100, 1, 32]             544\n",
      "        LeakyReLU-58           [-1, 100, 1, 32]               0\n",
      "           Linear-59           [-1, 100, 1, 64]           2,112\n",
      "        LeakyReLU-60           [-1, 100, 1, 64]               0\n",
      "           Linear-61           [-1, 100, 1, 64]           4,160\n",
      "        LeakyReLU-62           [-1, 100, 1, 64]               0\n",
      "           Linear-63           [-1, 100, 1, 16]           1,040\n",
      "           Linear-64           [-1, 100, 1, 32]             544\n",
      "        LeakyReLU-65           [-1, 100, 1, 32]               0\n",
      "           Linear-66           [-1, 100, 1, 64]           2,112\n",
      "        LeakyReLU-67           [-1, 100, 1, 64]               0\n",
      "           Linear-68           [-1, 100, 1, 64]           4,160\n",
      "        LeakyReLU-69           [-1, 100, 1, 64]               0\n",
      "           Linear-70           [-1, 100, 1, 16]           1,040\n",
      "           Linear-71           [-1, 100, 1, 32]             544\n",
      "        LeakyReLU-72           [-1, 100, 1, 32]               0\n",
      "           Linear-73           [-1, 100, 1, 64]           2,112\n",
      "        LeakyReLU-74           [-1, 100, 1, 64]               0\n",
      "           Linear-75           [-1, 100, 1, 64]           4,160\n",
      "        LeakyReLU-76           [-1, 100, 1, 64]               0\n",
      "           Linear-77           [-1, 100, 1, 16]           1,040\n",
      "           Linear-78           [-1, 100, 1, 32]             544\n",
      "        LeakyReLU-79           [-1, 100, 1, 32]               0\n",
      "           Linear-80           [-1, 100, 1, 64]           2,112\n",
      "        LeakyReLU-81           [-1, 100, 1, 64]               0\n",
      "           Linear-82           [-1, 100, 1, 64]           4,160\n",
      "        LeakyReLU-83           [-1, 100, 1, 64]               0\n",
      "           Linear-84           [-1, 100, 1, 16]           1,040\n",
      "           Conv2d-85           [-1, 32, 31, 31]           2,080\n",
      "      BatchNorm2d-86           [-1, 32, 31, 31]              64\n",
      "        LeakyReLU-87           [-1, 32, 31, 31]               0\n",
      "           Conv2d-88           [-1, 64, 14, 14]          32,832\n",
      "        LeakyReLU-89           [-1, 64, 14, 14]               0\n",
      "           Conv2d-90             [-1, 64, 6, 6]          65,600\n",
      "      BatchNorm2d-91             [-1, 64, 6, 6]             128\n",
      "        LeakyReLU-92             [-1, 64, 6, 6]               0\n",
      "           Conv2d-93             [-1, 48, 2, 2]          49,200\n",
      "        LeakyReLU-94             [-1, 48, 2, 2]               0\n",
      "           Conv2d-95           [-1, 32, 31, 31]           2,080\n",
      "      BatchNorm2d-96           [-1, 32, 31, 31]              64\n",
      "        LeakyReLU-97           [-1, 32, 31, 31]               0\n",
      "           Conv2d-98           [-1, 64, 14, 14]          32,832\n",
      "        LeakyReLU-99           [-1, 64, 14, 14]               0\n",
      "          Conv2d-100             [-1, 64, 6, 6]          65,600\n",
      "     BatchNorm2d-101             [-1, 64, 6, 6]             128\n",
      "       LeakyReLU-102             [-1, 64, 6, 6]               0\n",
      "          Conv2d-103             [-1, 48, 2, 2]          49,200\n",
      "       LeakyReLU-104             [-1, 48, 2, 2]               0\n",
      "         Flatten-105                  [-1, 192]               0\n",
      "         Flatten-106                  [-1, 192]               0\n",
      "          Linear-107                  [-1, 400]          77,200\n",
      "       LeakyReLU-108                  [-1, 400]               0\n",
      "          Linear-109                  [-1, 200]          80,200\n",
      "       LeakyReLU-110                  [-1, 200]               0\n",
      "          Linear-111                    [-1, 1]             201\n",
      "         Sigmoid-112                    [-1, 1]               0\n",
      "          Linear-113                  [-1, 400]          77,200\n",
      "       LeakyReLU-114                  [-1, 400]               0\n",
      "          Linear-115                  [-1, 200]          80,200\n",
      "       LeakyReLU-116                  [-1, 200]               0\n",
      "          Linear-117                    [-1, 1]             201\n",
      "         Sigmoid-118                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 692,194\n",
      "Trainable params: 692,194\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.81\n",
      "Forward/backward pass size (MB): 4.63\n",
      "Params size (MB): 2.64\n",
      "Estimated Total Size (MB): 11.09\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = SimilarityTensor()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.003)\n",
    "data_loader = dataset\n",
    "\n",
    "train_loss = []\n",
    "val_loss   = []\n",
    "\n",
    "summary(model, [(100,1,1),(100,1,1),(100,1,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "39fb187d-4b04-4ae7-a451-59608431da31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea98fe2ce3be46068452b19212216ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_epoch = 1\n",
    "\n",
    "for epoch in tqdm(range(n_epoch)):\n",
    "    model.train()\n",
    "    train_loss_per_epoch =[]\n",
    "    for i in range(len(data_loader[0])):\n",
    "        optimizer.zero_grad()\n",
    "        BT_dif, BF_dif = model(tokenizer(data_loader[0][i]),tokenizer(data_loader[1][i]),tokenizer(data_loader[2][i]))\n",
    "        loss = criterion(BT_dif, BF_dif)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_per_epoch.append(loss.item())\n",
    "        \n",
    "    train_loss.append(np.mean(train_loss_per_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0aeeceae-da17-4ed6-9d86-3c31dec452f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_loader[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0efe0d3-b908-40be-9347-0fb12fbc5230",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
